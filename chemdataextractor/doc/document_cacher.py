from .text import RichToken
import os
import shutil
import json


def get_document_configuration(document):
    # This assumes that there is at least one sentence in a document, and that the tokenizer is uniform across all elements.
    # We may want to change this.
    # We also don't store any information about sentence tokenization, which we may also want to do
    sentences = document.sentences
    document_configuration = {"tokenizer": sentences[0].word_tokenizer.__class__.__name__, "taggers": [tagger.__class__.__name__ for tagger in sentences[0].taggers]}
    return document_configuration


class PlainTextCacher:
    def __init__(self, cache_location):
        self.cache_location = cache_location
        if not os.path.isdir(self.cache_location):
            os.makedirs(self.cache_location)

    def cache_document(self, document, document_id, tags=None, save_subsentences=True, overwrite_cache=False):
        if tags is None:
            tags = ["ner_tag", "pos_tag"]

        # Save document configuration
        document_configuration = get_document_configuration(document)
        cache_location_root = os.path.join(self.cache_location, self._safe_document_id(document_id))
        if not os.path.isdir(cache_location_root):
            os.makedirs(cache_location_root)
        elif overwrite_cache:
            shutil.rmtree(cache_location_root)
            os.makedirs(cache_location_root)
        elif not overwrite_cache:
            raise AttributeError("{document_id} is already cached! Enable overwrite_cache to overwrite the previous cache.")

        with open(self._document_config_path(cache_location_root), "w+") as f:
            json.dump(document_configuration, f)

        sentences = document.sentences

        # Save tokenisation
        with open(self._document_tokenizer_cache_path(cache_location_root, document_configuration["tokenizer"]), "w+") as f:
            for sentence in sentences:
                # We use silly emoji to delineate so that we don't ever get conflicts with actual tokens used.
                # It's ðŸ™ƒ between words, and ðŸ”¥ between sentences
                f.write("ðŸ™ƒ".join(sentence.raw_tokens) + "ðŸ”¥")

        # Save start and end spans
        with open(self._document_tag_cache_path(cache_location_root, "start_end"), "w+") as f:
            for sentence in sentences:
                indices = [[token.start, token.end] for token in sentence.tokens]
                f.write(str(indices) + "\n")

        # Save tags
        for tag in tags:
            # Should we be more careful so that we always get the same tagger for each tag?
            with open(self._document_tag_cache_path(cache_location_root, tag), "w+") as f:
                for sentence in sentences:
                    # TODO(ti250): Assumes tags are plain text
                    sentence_tags = [token[tag] if token[tag] is not None else "ðŸ˜‚" for token in sentence.tokens]
                    f.write("ðŸ™ƒ".join(sentence_tags) + "ðŸ”¥")

    def hydrate_document(self, document, document_id, tags=None):
        # Add in all the tags, tokenisation for a document.
        document_configuration = get_document_configuration(document)
        cache_location_root = os.path.join(self.cache_location, self._safe_document_id(document_id))

        # Check cache looks good
        if not os.path.isdir(cache_location_root):
            raise AttributeError(f"{document_id} is not cached")

        with open(self._document_config_path(cache_location_root)) as f:
            cached_configuration = json.load(f)
            if cached_configuration["tokenizer"] != document_configuration["tokenizer"] or cached_configuration["taggers"] != document_configuration["taggers"]:
                raise AttributeError(f"Cached value for tokenizers and taggers don't match for {document_id}")

        sentences = document.sentences

        # Add tokenization
        with open(self._document_tokenizer_cache_path(cache_location_root, document_configuration["tokenizer"])) as f:
            # Need to remove the final element as it will be an empty one
            tokenized_sentences = [sent.split("ðŸ™ƒ") for sent in f.read().split("ðŸ”¥")][:-1]
            for sentence, tokenized_sentence in zip(sentences, tokenized_sentences):
                # TODO(ti250): We currently don't retain spans correctly here; what to do? We can always actually cache this...
                toks = [RichToken(
                    text=token,
                    start=-2,
                    end=-1,
                    lexicon=sentence.lexicon,
                    sentence=sentence
                ) for token in tokenized_sentence]
                sentence._tokens = toks

        # Correct start/end of tokens
        # Save start and end spans
        with open(self._document_tag_cache_path(cache_location_root, "start_end")) as f:
            all_indices = []
            for line in f.readlines():
                line = line.strip()
                if line != "":
                    all_indices.append(json.loads(line))
            for sentence, sent_indices in zip(sentences, all_indices):
                for token, indices in zip(sentence.tokens, sent_indices):
                    token.start = indices[0]
                    token.end = indices[1]

        # Add tags
        if tags is None:
            tags = ["ner_tag", "pos_tag"]

        for tag_type in tags:
            with open(self._document_tag_cache_path(cache_location_root, tag_type)) as f:
                all_tags = [sent.split("ðŸ™ƒ") for sent in f.read().split("ðŸ”¥")][:-1]
                for sentence, sentence_tags in zip(sentences, all_tags):
                    tokens = sentence.tokens
                    for token, tag in zip(tokens, sentence_tags):
                        token._tags[tag_type] = tag if tag != "ðŸ˜‚" else None

        return document

    def _safe_document_id(self, document_id):
        return document_id.replace(".", "ðŸ”¥")

    def _document_config_path(self, cache_location_root):
        return os.path.join(cache_location_root, "configuration.json")

    def _document_tokenizer_cache_path(self, cache_location_root, tokenizer):
        return os.path.join(cache_location_root, "tokenizer__" + tokenizer + ".txt")

    def _document_tag_cache_path(self, cache_location_root, tag):
        return os.path.join(cache_location_root, "tag__" + tag + ".txt")
